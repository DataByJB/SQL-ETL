# SQL-ETL: Data Transformation Process

## 📖 Overview
This project contains SQL scripts for an **ETL (Extract, Transform, Load) pipeline** designed to process and clean raw data before analysis. The project is structured in **three stages**:

1️⃣ **Stage 1 – Table Creation**  
   - Creates necessary tables for storing extracted data.  

2️⃣ **Stage 2 – Data Transformation**  
   - Cleans, formats, and transforms the data.

3️⃣ **Stage 3 – Data Loading**  
   - Loads transformed data into the final database tables.  

---

## 📂 Project Structure

📜 Stage 1 - Table Creation.sql │ 📜 Stage 2 - Data Transformation.sql │ 📜 Stage 3 - Data Loading.sql │


---

## 🛠️ Technologies Used
- **PostgreSQL** – Used for database management and SQL execution.  
- **VSCode** – For writing and managing SQL scripts.  
- **Git & GitHub** – Version control and project collaboration.

---

## 🌎 Application to other contexts
- Easily adaptable to process data in smaller, manageable chunks, making it simpler to scale across multiple servers or data sources.
- The ETL pipeline is not limited to a single domain, allowing for seamless data integration and analysis across various fields such as finance, healthcare, and retail.
- Designed to be flexible, allowing for easy customization and future improvements, such as automated scheduling and the addition of real time data quality checks.
- Effectively extracts, transforms, and loads data with built-in checks to ensure the datasets are accurate, clean, and ready for analysis and reporting.

---

## 📝 Author
- 👤 Jamal B
- 📧 [LinkedIn](https://www.linkedin.com/in/jamal-bartley-203860127/)
- 🌐 [Visit my portfolio](https://databyjb.carrd.co)

